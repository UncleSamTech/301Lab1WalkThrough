{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWg3e/txSLaGG9jExcqXDb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UncleSamTech/301Lab1WalkThrough/blob/main/bilstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hXby6tw1I10S"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score,confusion_matrix, classification_report\n",
        "import pickle\n",
        "import time\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from random import sample\n",
        "import seaborn as sns\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import heapq\n",
        "import cProfile\n",
        "import pstats"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_token_score(context, token, tokenz, model, maxlen):\n",
        "        # Early check for out-of-vocabulary token\n",
        "        LOW_SCORE =  -1\n",
        "        token_index = tokenz.word_index.get(token, LOW_SCORE)\n",
        "        if token_index == LOW_SCORE:\n",
        "            return LOW_SCORE  # Assign low score for out-of-vocabulary token\n",
        "\n",
        "        # Tokenize context\n",
        "        context_sequence = tokenz.texts_to_sequences([context])[0]\n",
        "        if not context_sequence or not context_sequence[0]:\n",
        "            raise ValueError(\"Context could not be tokenized correctly.\")\n",
        "\n",
        "        # Append token index to context\n",
        "        token_value = context_sequence + [token_index]\n",
        "\n",
        "        # Ensure the input is the correct length\n",
        "        # if len(token_value) < maxlen - 1:\n",
        "        #     token_value = [0] * (maxlen - 1 - len(token_value)) + token_value\n",
        "        # else:\n",
        "        #     token_value = token_value[-(maxlen-1):]\n",
        "\n",
        "        token_value = pad_sequences([token_value], maxlen=maxlen-1, padding='pre', truncating='pre')[0]\n",
        "\n",
        "        # Prepare input as a list\n",
        "        padded_in_seq = [token_value]\n",
        "\n",
        "        # Model prediction\n",
        "        prediction = model.predict(padded_in_seq, verbose=1)\n",
        "        if len(prediction[0]) == 0:\n",
        "            raise ValueError(\"Prediction output is empty.\")\n",
        "        return prediction[0][-1]  # Score of the token\n"
      ],
      "metadata": {
        "id": "eJtN3xV8LPln"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_bilstm_mrr_single_file(filename, maxlen, model, result_path):\n",
        "        \"\"\"\n",
        "        Evaluate the MRR for a single file.\n",
        "        Save the total reciprocal rank and total lines for each file to a text file.\n",
        "        \"\"\"\n",
        "\n",
        "        ld = load_model(model,compile=False)\n",
        "        #ld.summary()\n",
        "        # Load the tokenizer\n",
        "        with open(os.path.join(result_path, \"tokenized_file_50embedtime1.pickle\"), \"rb\") as tk:\n",
        "            tokenz = pickle.load(tk)\n",
        "\n",
        "        vocab = list(tokenz.word_index.keys())\n",
        "\n",
        "        # Ensure result path exists\n",
        "        os.makedirs(result_path, exist_ok=True)\n",
        "\n",
        "        # # Process each file in the split folder\n",
        "        # for split_file in sorted(os.listdir(split_folder)):\n",
        "        #     split_file_path = os.path.join(split_folder, split_file)\n",
        "        #     if not os.path.isfile(split_file_path):\n",
        "        #         continue\n",
        "\n",
        "        total_cumulative_rr = 0\n",
        "        total_count = 0\n",
        "        #profiler = cProfile.Profile()\n",
        "        #profiler.enable()\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "            # Process each line in the file\n",
        "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                if not line.strip():\n",
        "                    continue\n",
        "\n",
        "                # Preprocess the line\n",
        "                #line = line.replace(\"_\", \"UNDERSCORE\").replace(\">\", \"RIGHTANG\").replace(\"<\", \"LEFTANG\").lower()\n",
        "                sentence_tokens = line.split(\" \")\n",
        "                if len(sentence_tokens) < 2:\n",
        "                    continue\n",
        "\n",
        "                context = \" \".join(sentence_tokens[:-1])\n",
        "                true_next_word = sentence_tokens[-1]\n",
        "\n",
        "                # Compute scores for tokens\n",
        "                heap = []\n",
        "                for token in vocab:\n",
        "                    context_score = predict_token_score(context, token, tokenz, ld, maxlen)\n",
        "                    if len(heap) < 10:\n",
        "                        heapq.heappush(heap, (context_score, token))\n",
        "                    elif context_score > heap[0][0]:\n",
        "                        heapq.heappushpop(heap, (context_score, token))\n",
        "\n",
        "                heap.sort(reverse=True, key=lambda x: x[0])\n",
        "                token_ranks = {t: rank + 1 for rank, (score, t) in enumerate(heap)}\n",
        "\n",
        "                # Compute reciprocal rank\n",
        "                true_next_word = true_next_word.strip()\n",
        "                rank = token_ranks.get(true_next_word, 0)\n",
        "                if rank:\n",
        "                    current_rank = 1 / rank\n",
        "                    total_cumulative_rr += current_rank\n",
        "                    #print(f\"processed line  context {context} with rank {current_rank} and tcr {total_cumulative_rr}\")\n",
        "                total_count += 1\n",
        "\n",
        "            #profiler.disable()\n",
        "\n",
        "            # Save profiling results to a file\n",
        "            #profile_file = os.path.join(result_path, f\"evalmrrvisib.prof\")\n",
        "            #with open(profile_file, \"w\") as pf:\n",
        "                #stats = pstats.Stats(profiler, stream=pf)\n",
        "                #stats.sort_stats('cumulative')\n",
        "                #stats.print_stats()\n",
        "\n",
        "            # Calculate total RR and lines for the file\n",
        "            time_spent = time.time() - start_time\n",
        "            result_file = os.path.join(result_path, f\"kenlm_results_file_{filename}.txt\")\n",
        "\n",
        "        with open(result_file, \"a\") as rf:\n",
        "            rf.write(f\"File name : {filename}\\n\")\n",
        "            rf.write(f\"Total Reciprocal Rank: {total_cumulative_rr}\\n\")\n",
        "            rf.write(f\"Total Lines: {total_count}\\n\")\n",
        "            rf.write(f\"Time Spent: {time_spent:.2f} seconds\\n\")\n"
      ],
      "metadata": {
        "id": "JQOQYsFRLhqB"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_bilstm_mrr_single_file(\"/content/scratch_test_data_chunk_1.txt\",33,\"/content/main_bilstm_scratch_model_150embedtime_100.keras\",\"/content/result_path/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "AA1PWwr5LvqD",
        "outputId": "ccf9d6c9-f57e-4bbc-a588-5375704408de"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"sequential_1_1/Cast:0\", shape=(32,), dtype=float32). Expected shape (None, 38), but input has incompatible shape (32,)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=('tf.Tensor(shape=(32,), dtype=int32)',)\n  • training=False\n  • mask=('None',)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-26a647079f0a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_bilstm_mrr_single_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/scratch_test_data_chunk_1.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/content/main_bilstm_scratch_model_150embedtime_100.keras\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/content/result_path/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-d758bba15889>\u001b[0m in \u001b[0;36mevaluate_bilstm_mrr_single_file\u001b[0;34m(filename, maxlen, model, result_path)\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mheap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                     \u001b[0mcontext_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_token_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mld\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheap\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                         \u001b[0mheapq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheappush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcontext_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-1ef29413e3ac>\u001b[0m in \u001b[0;36mpredict_token_score\u001b[0;34m(context, token, tokenz, model, maxlen)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Model prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_in_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Prediction output is empty.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36m_adjust_input_rank\u001b[0;34m(self, flat_inputs)\u001b[0m\n\u001b[1;32m    242\u001b[0m                     \u001b[0madjusted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    245\u001b[0m                 \u001b[0;34mf\"Invalid input shape for input {x}. Expected shape \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0;34mf\"{ref_shape}, but input has incompatible shape {x.shape}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"sequential_1_1/Cast:0\", shape=(32,), dtype=float32). Expected shape (None, 38), but input has incompatible shape (32,)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=('tf.Tensor(shape=(32,), dtype=int32)',)\n  • training=False\n  • mask=('None',)"
          ]
        }
      ]
    }
  ]
}